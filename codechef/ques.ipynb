{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "5 \n",
      "8 7 8 9 9\n",
      "8 7\n",
      "8 8\n",
      "8 9\n",
      "8 9\n",
      "7 8\n",
      "7 9\n",
      "7 9\n",
      "8 9\n",
      "8 9\n",
      "9 9\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations \n",
    "for h in range(int(input())):\n",
    "    \n",
    "  days = int(input())\n",
    "  price = list(map(int,input().split()))\n",
    "\n",
    "  profit = 0\n",
    "  buy = 0\n",
    "  sell = 0\n",
    "  \n",
    "  for j in range(days):\n",
    "    for c in range(j+1,days):\n",
    "        print(price[j],price[c])\n",
    "        buy = price[j]\n",
    "        sell = price[c]\n",
    "        temp = sell - buy\n",
    "        if profit<temp:\n",
    "          profit = temp\n",
    "    \n",
    "  print(profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 7\n",
      "8 7\n",
      "8 5\n",
      "8 5\n",
      "7 7\n",
      "7 5\n",
      "7 5\n",
      "7 5\n",
      "7 5\n",
      "5 5\n"
     ]
    }
   ],
   "source": [
    "q=list(combinations([8,7,7,5,5], 2))\n",
    "for j,w in q:\n",
    "    print(j,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "d=[(2,3),(2,3)]\n",
    "s=set(d)\n",
    "for j in s:\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2,3) in d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df;dfdf\n",
      "['df', 'dfdf']\n"
     ]
    }
   ],
   "source": [
    "d = input().split(\";\")\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'dom'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-28ce409565f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m  \u001b[0;34m<\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m </response>\"\"\"\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminidom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'dom'"
     ]
    }
   ],
   "source": [
    "xml = \"\"\"<response>\n",
    " <article>\n",
    "  <title>A Novel Approach to Image Classification, in a Cloud Computing Environment stability.</title>\n",
    "  <publicationtitle>IEEE Transactions on Cloud Computing</publicationtitle>\n",
    "  <abstract>Classification of items within PDF documents has always been challenging.  This stability document will discuss a simple classification algorithm for indexing images within a PDF.</abstract>\n",
    " </article>\n",
    " <body>\n",
    "  <sec>\n",
    "   <label>I.</label>\n",
    "   <p>Should Haven't That is a bunch of text pattern these classification and cyrptography.  These paragraphs are nothing but nonsense.  What is the statbility of your program to find neural nets. Throw in some numbers to see if you get the word count correct this is a classification this in my nd and rd words.  What the heck throw in cryptography.</p>\n",
    "   <p>I bet diseases you can't find probability twice.  Here it is a again probability.  Just to fool you I added it three times probability.  Does this make any pattern classification? pattern classification! pattern classification.</p>\n",
    "   <p>\n",
    "    <fig>\n",
    "     <label>FIGURE.</label>\n",
    "     <caption>This is a figure representing convolutional neural nets.</caption>\n",
    "    </fig>\n",
    "   </p>\n",
    " </sec>\n",
    " </body>\n",
    "</response>\"\"\"\n",
    "c=xml.dom.minidom.parse(xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 5 5 15\n",
      "[5, 5, 5, 0]\n"
     ]
    }
   ],
   "source": [
    "C,N,P,W = input().split()\n",
    "C=int(C)\n",
    "N=int(N)\n",
    "P=int(P)\n",
    "W=int(W)\n",
    "d=[]\n",
    "d.append(P)\n",
    "while P:\n",
    "    if len(d)>N:\n",
    "        break\n",
    "    W-=P\n",
    "    if W<0:\n",
    "        break\n",
    "    if P>W:\n",
    "        d.append(W)\n",
    "    else:\n",
    "        d.append(P)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4 3 3 8\n",
      "W 8\n",
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "C,N,P,W = input().split()\n",
    "C=int(C)\n",
    "N=int(N)\n",
    "P=int(P)\n",
    "W=int(W)\n",
    "tables=0\n",
    "while (W/C > 0):\n",
    "    print(\"W\",W)\n",
    "    \n",
    "    W-=C\n",
    "    tables+=1\n",
    "    N=N-1\n",
    "    print((math.ceil(W/P)+tables)>N)\n",
    "    if (math.ceil(W/P)+tables>N):\n",
    "        break\n",
    "\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-a159603e406e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \"\"\"\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'punkt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gutenberg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Untitled11.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1P2ApTC-VZIMnZFnxb5g1N9rUFKsrnl0B\n",
    "\"\"\"\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "sample_data = str(input())\n",
    "updated_data = sample_data.replace(';','\\n')\n",
    "\n",
    "sample_data_1 = str(input())\n",
    "updated_data_1 = sample_data_1.replace(';','\\n')\n",
    "\n",
    "xml = str(input())\n",
    "import re\n",
    "data_confer = re.sub(r'<.*?>','', xml)\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "data_confer = sent_tokenize(data_confer)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def get_clean_sentences(sentences, remove_digits=False):\n",
    "    '''Cleaning sentences by removing special characters and optionally digits'''\n",
    "    clean_sentences = []\n",
    "    for sent in sentences:\n",
    "        pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]' \n",
    "        clean_text = re.sub(pattern, '', sent)\n",
    "        clean_text = clean_text.lower()  # Converting to lower case\n",
    "        clean_sentences.append(clean_text)\n",
    "    return clean_sentences\n",
    "\n",
    "data_confer_1 = get_clean_sentences(data_confer, remove_digits = True)\n",
    "\n",
    "word_tokens = nltk.word_tokenize(updated_data)\n",
    "\n",
    "word_tokens_1 = nltk.word_tokenize(updated_data_1)\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def get_word_tokens(sentences):\n",
    "    '''Word tokenization'''\n",
    "    words = []\n",
    "    for sent in sentences:\n",
    "        words.extend(word_tokenize(sent))\n",
    "    return(words)\n",
    "\n",
    "conference_words = get_word_tokens(data_confer_1)\n",
    "\n",
    "def filter_stopwords(words):\n",
    "    '''Removing stopwords from given words'''\n",
    "    filtered_words = [w for w in words if w not in stop_words]\n",
    "    return filtered_words\n",
    "\n",
    "nltk_tokens = filter_stopwords(conference_words)\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def get_pos_tags(words):\n",
    "    '''Get the part of speech (POS) tags for the words'''\n",
    "    tags=[]\n",
    "    for word in words:\n",
    "        tags.append(nltk.pos_tag([word]))\n",
    "    return tags\n",
    "\n",
    "sample_tags = get_pos_tags(word_tokens)\n",
    "\n",
    "sample_tags_1 = get_pos_tags(word_tokens_1)\n",
    "\n",
    "conference_tags = get_pos_tags(nltk_tokens)\n",
    "\n",
    "def intersection(lst1, lst2): \n",
    "    lst3 = [value for value in lst1 if value in lst2] \n",
    "    return lst3 \n",
    "\n",
    "lst1 = nltk_tokens\n",
    "lst2 = word_tokens_1\n",
    "final_list = intersection(lst1, lst2)\n",
    "\n",
    "from collections import Counter\n",
    "counts = Counter(final_list)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "being;does;have;haven't;more;should;shouldn't;than;that;these;what\n",
      "classification;cryptography;diseases;probability;stability\n",
      "<response>  <article>   <title>A Novel Approach to Image Classification, in a Cloud Computing Environment stability.</title>   <publicationtitle>IEEE Transactions on Cloud Computing</publicationtitle>   <abstract>Classification of items within PDF documents has always been challenging.  This stability document will discuss a simple classification algorithm for indexing images within a PDF.</abstract>  </article>  <body>   <sec>    <label>I.</label>    <p>Should Haven't That is a bunch of text pattern these classification and cyrptography.  These paragraphs are nothing but nonsense.  What is the statbility of your program to find neural nets. Throw in some numbers to see if you get the word count correct this is a classification this in my nd and rd words.  What the heck throw in cryptography.</p>    <p>I bet diseases you can't find probability twice.  Here it is a again probability.  Just to fool you I added it three times probability.  Does this make any pattern classification? pattern classification! pattern classification.</p>    <p>     <fig>      <label>FIGURE.</label>      <caption>This is a figure representing convolutional neural nets.</caption>     </fig>    </p>  </sec>  </body> </response>\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'classification'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-a1544ad193eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mdicti\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindT1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mindT2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mdicti\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindA1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mindA2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mdicti\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindB1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mindB2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'classification'"
     ]
    }
   ],
   "source": [
    "l1=list(input().split(';'))\n",
    "l2=list(input().split(';'))\n",
    "file=input()\n",
    "length=file.length()\n",
    "\n",
    "indT1 = file.find(\"<title>\")\n",
    "indT2 = file.find(\"</title>\")\n",
    "indA1 = file.find(\"<abstract>\")\n",
    "indA2 = file.find(\"</abstract>\")\n",
    "indB1 = file.find(\"<body>\")\n",
    "indB2 = file.find(\"</body>\")\n",
    "dicti = dict()\n",
    "\n",
    "for i in l2:\n",
    "    dicti[i] += file[indT1:indT2+1:].count(i)\n",
    "    dicti[i] += 3file[indA1:indA2+1:].count(i)\n",
    "    dicti[i] += file[indB1:indB2+1:].count(i)\n",
    "\n",
    "sort_dict = sorted(dicti.items(),key = lambda x: x[1],reverse = True)\n",
    "seti = set()\n",
    "ind = 0\n",
    "while(ind <len(l2) and len(seti)!=3 ):\n",
    "    seti.add(sort_dict.values()[ind])\n",
    "    ind +=1\n",
    "\n",
    "# ans = dict()\n",
    "for stre in sort_dict:\n",
    "    if(ind):\n",
    "        print(stre[0]+\":\",stre[1])\n",
    "    else:\n",
    "        break\n",
    "    ind -=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "[('ieee', 20), ('xtreme', 15), ('welcome', 9), ('ieeextreme', 8), ('programming', 3)]\n",
      "ieee :  55.55555555555556\n",
      "xtreme :  41.66666666666667\n",
      "welcome :  25.0\n",
      "ieeextreme :  22.22222222222222\n",
      "programming :  8.333333333333332\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "l1=\"what;when;where;like;that\".split(';')\n",
    "l2=\"welcome;ieee;xtreme;ieeextreme;programming\".split(';')\n",
    "# while True:\n",
    "#   try:\n",
    "#     file1 = input()\n",
    "#     if len(file1)==0:\n",
    "#         break\n",
    "#     file+=file1\n",
    "#   except Exception as e:\n",
    "#     pass\n",
    "file = \"\"\"\n",
    "    <title>Welcome to IEEEXtreme!</title>\n",
    "<keyword>welcome, ieeextreme</keyword>\n",
    "<abstract>\n",
    "Welcome!Participants!!!\n",
    "IEEE Xtreme is a global challenge in which teams of IEEE Student members, advised and proctored by an IEEE member, and often supported by an IEEE Student Branch.\n",
    "Compete in a twentyfour hour timespan against each other to solve a set of ...XtremE... programming problems in IEEEXtreme.\n",
    "</abstract>\n",
    "<body>WELCOME. wel.come... Are you ready? Good luck and have fun in xtreme!</body>\n",
    "<other>\n",
    "Mark your calender and don't miss the action.\n",
    "</other>\n",
    "    \"\"\"\n",
    "length=len(file)\n",
    "file = file.lower()\n",
    "indT1 = file.find(\"<title>\")\n",
    "indT2 = file.find(\"</title>\")\n",
    "indA1 = file.find(\"<abstract>\")\n",
    "indA2 = file.find(\"</abstract>\")\n",
    "indB1 = file.find(\"<body>\")\n",
    "indB2 = file.find(\"</body>\")\n",
    "dicti = {}\n",
    "\n",
    "for c in l2:\n",
    "    dicti[c]=0\n",
    "le = file[indT1:indT2]\n",
    "# data_confer = re.sub(r'<.*?>','', le)\n",
    "# le=data_confer.split()\n",
    "\n",
    "le1 = file[indA1:indA2]\n",
    "# data_confer = re.sub(r'<.*?>','', le1)\n",
    "# le1=data_confer.split()\n",
    "\n",
    "le2 = file[indB1:indB2]\n",
    "\n",
    "# le2 = data_confer.split()\n",
    "\n",
    "\n",
    "# # print(le3)\n",
    "finallist=le+le1+le2\n",
    "da = re.sub(r'<.*?>','', finallist)\n",
    "da=re.sub('[^a-z\\ ]+', '', da)\n",
    "# print(da)\n",
    "finallist = da.split()\n",
    "# print(len(finallist))\n",
    "finallist1 = finallist\n",
    "for v in l1:\n",
    "    for j in finallist1:\n",
    "        if v==j:\n",
    "            finallist.remove(j)\n",
    "      \n",
    "    \n",
    "# print(len(finallist))\n",
    "ss = 0\n",
    "\n",
    "# print(len(finallist))\n",
    "for b in finallist:\n",
    "    if len(b)>=4:\n",
    "        ss+=1\n",
    "        \n",
    "#     if  b=='\\n':\n",
    "#         finallist.remove(b)\n",
    "#         ss-=1\n",
    "#     if '!' in b or b=='!':\n",
    "#         ss-=1\n",
    "#     if b==' ':\n",
    "#         finallist.remove(b)\n",
    "#         ss-=1\n",
    "#     if '.' in b or b=='.':\n",
    "#         ss-=1\n",
    "        \n",
    "print(ss)\n",
    "\n",
    "\n",
    "for j in l2:\n",
    "    if j in file[indT1:indT2]:\n",
    "        dicti[j] = file[indT1:indT2].count(j)*5\n",
    "    if j in file[indA1:indA2]:\n",
    "        dicti[j] += file[indA1:indA2].count(j)*3\n",
    "    if j in file[indB1:indB2]:\n",
    "        dicti[j] += file[indB1:indB2].count(j)\n",
    "# print(dicti)\n",
    "sort_orders = sorted(dicti.items(), key=lambda x: x[1], reverse=True)\n",
    "print(sort_orders)\n",
    "for j in sort_orders:\n",
    "    print(j[0],\": \",(j[1]/ss)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcbaabc\n",
      "bcb \t aa\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from itertools import combinations\n",
    "def get_combination(string):\n",
    "    string_array = []\n",
    "    for y in range(len(string)-1,1,-1):\n",
    "        for x in combinations(string,y):\n",
    "            if ''.join(x)==''.join(x)[::-1]:\n",
    "                string_array.append(''.join(x))\n",
    "                break\n",
    "    return string_array\n",
    "def funPal(string):\n",
    "    palindrome_len = []\n",
    "    for i in range(0, len(string)):\n",
    "        str1_array = []\n",
    "        str2_array = []\n",
    "        string1 = string[:i]\n",
    "        string2 = string[i:]\n",
    "        str1_array = get_combination(string1)\n",
    "        str2_array = get_combination(string2)\n",
    "        if str1_array and str2_array:\n",
    "            print (str1_array[0], \"\\t\", str2_array[0])\n",
    "            palindrome_len.append(len(str1_array[0]) * len(str2_array[0]))\n",
    "    return max(palindrome_len)\n",
    "        \n",
    "_s = input()\n",
    "res = funPal(_s);\n",
    "print (res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-150-c00303e1d561>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mdata_confer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'<.*?>'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0msent_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_confer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Untitled12.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1vxhPsMd4lTz61PAUmhjKCPQwUWNKZuk1\n",
    "\"\"\"\n",
    "\n",
    "file=\"\"\"<response>\n",
    " <article>\n",
    "  <title>A Novel Approach to Image Classification, in a Cloud Computing Environment stability.</title>\n",
    "  <publicationtitle>IEEE Transactions on Cloud Computing</publicationtitle>\n",
    "  <abstract>Classification of items within PDF documents has always been challenging.  This stability document will discuss a simple classification algorithm for indexing images within a PDF.</abstract>\n",
    " </article>\n",
    " <body>\n",
    "  <sec>\n",
    "   <label>I.</label>\n",
    "   <p>Should Haven't That is a bunch of text pattern these classification and cyrptography.  These paragraphs are nothing but nonsense.  What is the statbility of your program to find neural nets. Throw in some numbers to see if you get the word count correct this is a classification this in my nd and rd words.  What the heck throw in cryptography.</p>\n",
    "   <p>I bet diseases you can't find probability twice.  Here it is a again probability.  Just to fool you I added it three times probability.  Does this make any pattern classification? pattern classification! pattern classification.</p>\n",
    "   <p>\n",
    "    <fig>\n",
    "     <label>FIGURE.</label>\n",
    "     <caption>This is a figure representing convolutional neural nets.</caption>\n",
    "    </fig>\n",
    "   </p>\n",
    " </sec>\n",
    " </body>\n",
    "</response>\"\"\"\n",
    "\n",
    "import re\n",
    "data_confer = re.sub(r'<.*?>','', file)\n",
    "\n",
    "import nltk\n",
    "sent_tokens = nltk.sent_tokenize(data_confer)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def get_clean_sentences(sentences, remove_digits=False):\n",
    "    '''Cleaning sentences by removing special characters and optionally digits'''\n",
    "    clean_sentences = []\n",
    "    for sent in sentences:\n",
    "        pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]' \n",
    "        clean_text = re.sub(pattern, '', sent)\n",
    "        clean_text = clean_text.lower()  # Converting to lower case\n",
    "        clean_sentences.append(clean_text)\n",
    "    return clean_sentences\n",
    "\n",
    "def filter_stopwords(words):\n",
    "    '''Removing stopwords from given words'''\n",
    "    filtered_words = [w for w in words if w not in stop_words]\n",
    "    print('\\nFiltered words:', filtered_words)\n",
    "    return filtered_words\n",
    "\n",
    "conference_sentences = get_clean_sentences(sent_tokens)\n",
    "conference_words = get_word_tokens(conference_sentences)\n",
    "conf_words = filter_stopwords(conference_words)\n",
    "\n",
    "length_para = len(conf_words)\n",
    "length_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-203-d4ba86949ae0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stopwords'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "l2=\"classification;cryptography;diseases;probability;stability\".split(';')\n",
    "l1=\"being;does;have;haven't;more;should;shouldn't;than;that;these;what\".split(';')\n",
    "file=\"\"\"<response>\n",
    " <article>\n",
    "  <title>A Novel Approach to Image Classification, in a Cloud Computing Environment stability.</title>\n",
    "  <publicationtitle>IEEE Transactions on Cloud Computing</publicationtitle>\n",
    "  <abstract>Classification of items within PDF documents has always been challenging.  This stability document will discuss a simple classification algorithm for indexing images within a PDF.</abstract>\n",
    " </article>\n",
    " <body>\n",
    "  <sec>\n",
    "   <label>I.</label>\n",
    "   <p>Should Haven't That is a bunch of text pattern these classification and cyrptography.  These paragraphs are nothing but nonsense.  What is the statbility of your program to find neural nets. Throw in some numbers to see if you get the word count correct this is a classification this in my nd and rd words.  What the heck throw in cryptography.</p>\n",
    "   <p>I bet diseases you can't find probability twice.  Here it is a again probability.  Just to fool you I added it three times probability.  Does this make any pattern classification? pattern classification! pattern classification.</p>\n",
    "   <p>\n",
    "    <fig>\n",
    "     <label>FIGURE.</label>\n",
    "     <caption>This is a figure representing convolutional neural nets.</caption>\n",
    "    </fig>\n",
    "   </p>\n",
    " </sec>\n",
    " </body>\n",
    "</response>\"\"\"\n",
    "length=len(file)\n",
    "file = file.lower()\n",
    "indT1 = file.find(\"<title>\")\n",
    "indT2 = file.find(\"</title>\")\n",
    "indA1 = file.find(\"<abstract>\")\n",
    "indA2 = file.find(\"</abstract>\")\n",
    "indB1 = file.find(\"<body>\")\n",
    "indB2 = file.find(\"</body>\")\n",
    "dicti = {}\n",
    "for c in l2:\n",
    "    dicti[c]=0\n",
    "le = file[indT1+7:indT2].split()\n",
    "le2 = file[indA1:indA2].split()\n",
    "le3 = file[indB1:indB2].split()\n",
    "finallist=le+le2+le3\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "data_confer = re.sub(r'<.*?>','', file)\n",
    "def clean_text(data_confer):\n",
    "    text = data_confer.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n",
    "final = pattern.sub('', data_confer)\n",
    "spaces = r\"\\s+\"\n",
    "list_up = re.split(spaces, final)\n",
    "\n",
    "ss = len(list_up)\n",
    "\n",
    "if '' in list_up:\n",
    "  ss-=1\n",
    "\n",
    "for j in l2:\n",
    "    if j in file[indT1:indT2]:\n",
    "        dicti[j] = file[indT1:indT2].count(j)*5\n",
    "    if j in file[indA1:indA2]:\n",
    "        dicti[j] += file[indA1:indA2].count(j)*3\n",
    "    if j in file[indB1:indB2]:\n",
    "        dicti[j] += file[indB1:indB2].count(j)\n",
    "sort_orders = sorted(dicti.items(), key=lambda x: x[1], reverse=True)\n",
    "for j in sort_orders[:3]:\n",
    "    print(j[0],\": \",(j[1]/ss)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 3 3 8\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "tWd,nPt,pWd,wTl = input().split()\n",
    "tWd=int(tWd)\n",
    "nPt=int(nPt)\n",
    "pWd=int(pWd)\n",
    "wTl=int(wTl)\n",
    "tab=0\n",
    "while(wTl//tWd > 0):\n",
    "    wTl -= tWd\n",
    "    tab+=1\n",
    "    if(math.ceil(wTl/pWd)+tab >= nPt):\n",
    "        break\n",
    "print(tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2 2\n",
      "0 0 0\n",
      "Error\n"
     ]
    }
   ],
   "source": [
    "# Let c, h, o be the number of carbon, hydrogen and oxygen atoms respectively\n",
    "c, h, o = map(int, input().split())\n",
    "\n",
    "# Let the number of water molecules be x, number of carbondioxide molecules be y\n",
    "# and the number of glucose molecules be z.\n",
    "\n",
    "y = (2 * o - h) / 4\n",
    "x = (h - 2 * c + 2 * y) / 2\n",
    "z = (c - y) / 6\n",
    "print(int(x), int(y), int(z))\n",
    "# we can't have remaining atoms of any type after creating the molecules therefore we have to check whether the\n",
    "# values of x, y and z contain any decimal values. If there are decimal values then the number of atoms are inconsistent and\n",
    "# we have to print \"Error\"\n",
    "if x < 0 or (int(x) != x):\n",
    "    print(\"Error\")\n",
    "elif y < 0 or (int(y) != y):\n",
    "    print(\"Error\")\n",
    "elif z < 0 or (int(z) != z):\n",
    "    print(\"Error\")\n",
    "else:\n",
    "    print(int(x), int(y), int(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 8\n",
      "Zulian 3 100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'Zulian'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-229-e9fd6774d20e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mcoin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcoin\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoin\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'Zulian'"
     ]
    }
   ],
   "source": [
    "N,M=input().split()\n",
    "N=int(N)\n",
    "M=int(M)\n",
    "d={}\n",
    "for j in range(M):\n",
    "    coin=list(map(int,input().split()))\n",
    "    d[coin]=coin[1:]\n",
    "print(d)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "https://www.geeksforgeeks.org/python-program-to-find-uncommon-words-from-two-strings/?ref=rp\n",
      "['https://www.geeksforgeeks.org/python-program-to-find-uncommon-words-from-two-strings/?ref=rp']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# 6\n",
    "# WELC-OMET-OTHE\n",
    "# IEEE-XTRE-ME14\n",
    "# AAAA-0000-A0A0\n",
    "# AAAA-0000-A0A1\n",
    "# AAAA-0000-A0AB\n",
    "# AAAA-0000-ABAB\n",
    "def hammingDist(str1, str2):\n",
    "    i = 0\n",
    "    count = 0\n",
    " \n",
    "    while(i < len(str1)):\n",
    "        if(str1[i] != str2[i]):\n",
    "            count += 1\n",
    "        i += 1\n",
    "    return count\n",
    " \n",
    "# Driver code  \n",
    "d=[]\n",
    "for h in range(int(input())):\n",
    "    x=input()\n",
    "    d.append(x)\n",
    "print(d)\n",
    "for j in range(len(d)):\n",
    "    for k in \n",
    "print(hammingDist(str1, str2))\n",
    " \n",
    "# This code is contributed by avanitrachhadiya21551\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
